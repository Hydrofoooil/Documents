###  `loss.backward()` 的工作原理

1. 两个构建计算图:
   当代码执行前向传播（forward pass），即从输入数据开始，经过模型的每一层（Embedding, Linear等），直到计算出最终的 loss 值时，PyTorch 会在后台自动构建一个**计算图 (Computation Graph)**。这个图记录了所有张量以及在它们上面执行的所有数学运算（加法、乘法、矩阵乘法等）的依赖关系。图的终点就是最终的标量损失值 loss。
2. 反向传播与链式法则:
   当调用 loss.backward() 时，PyTorch 会从计算图的终点（loss）开始，沿着图反向传播，用链式法则来计算 loss 相对于图中每一个参数（**即设置了 requires_grad=True 的张量**，主要是模型的权重和偏置）的梯度。
3. 存储梯度:
   计算完成后，每个参数张量（例如 model.fc.weight）的梯度值会被累加并存储在它自身的 .grad 属性中。例如，model.fc.weight.grad 将会包含 loss 对 model.fc.weight 的梯度。


### `optimizer.step()` 的工作原理

它的工作发生在 `loss.backward()` 之后，并且依赖于 `loss.backward()` 计算出的梯度。

以下是它的工作流程：

1. **前提条件** : `loss.backward()` 已经被调用。这意味着模型中所有需要更新的参数（权重和偏置）的 `.grad` 属性现在都存储了对应的梯度值。
2. **遍历参数** : 当 `optimizer.step()` 被调用时，优化器会遍历它在初始化时收到的所有参数（即 `model.parameters()`）。
3. **执行更新规则** : 对于每一个参数，优化器会根据其自身的更新规则（update rule）来调整参数的值。最基础的更新规则是 **随机梯度下降（SGD）** ，其公式为：
   **new_weight**=**old_weight**−**learning_rate**×**gradient**

* `old_weight`: 参数当前的值。
* `learning_rate`: 学习率（在定义优化器时设置，例如 `lr=0.001`），它控制着每次更新的步长。
* `gradient`: 从该参数的 `.grad` 属性中获取的梯度值。

1. **特定优化器的复杂规则** : 在代码中使用的是 `Adam` 优化器。`Adam` 比基础的 SGD 更为复杂和高效。它除了使用上述基本规则外，还内部维护了每个参数的“动量”（momentum，过去梯度的移动平均值）和“速度”（velocity，过去梯度平方的移动平均值）。这使得 `Adam` 能够：

* 为模型中的每一个参数计算出 **自适应的学习率** 。
* 在更新时加入动量的概念，帮助加速收敛并越过一些小的局部最优点。

   因此，`Adam` 的 `step()` 函数执行的数学运算比简单的 SGD 要复杂，但其最终目的仍然是根据计算出的梯度，以一种更智能、更高效的方式来更新参数值。

**总结一下训练循环中的完整流程：**

1. `optimizer.zero_grad()`: 清除上一步迭代中旧的梯度信息。
2. `loss.backward()`: 计算当前批次数据下，损失函数关于每一个模型参数的梯度，并将这些梯度存入参数的 `.grad` 属性中。
3. `optimizer.step()`: 使用存储在 `.grad` 属性中的梯度，并根据 `Adam` 优化算法的更新规则，来调整所有模型参数的值，完成一次学习和优化。
